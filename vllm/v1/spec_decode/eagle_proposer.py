# SPDX-License-Identifier: Apache-2.0
import itertools

import torch
from torch import Tensor, nn

from vllm.config import VllmConfig
from vllm.forward_context import set_forward_context
from vllm.v1.attention.backends.flash_attn import FlashAttentionMetadata
from vllm.v1.outputs import SamplerOutput
from vllm.v1.sample.metadata import SamplingMetadata


class EagleProposer:

    def __init__(self, vllm_config: VllmConfig, model: nn.Module,
                 sampling_metadata: SamplingMetadata):
        """
        Initialize EagleProposer with necessary configuration and model.
        """
        self._vllm_config = vllm_config
        self._model = model
        self._sampling_metadata = sampling_metadata

    def generate_draft_token_ids(
            self, target_model_input_ids: Tensor,
            target_model_positions: Tensor, target_model_hidden_states: Tensor,
            target_model_seq_lens: list[int],
            sampled_token_ids: list[list[int]],
            next_prompt_token_ids: list[list[int]], is_prefill: list[bool],
            num_draft_tokens_to_propose: int,
            attention_metadata: FlashAttentionMetadata) -> list[SamplerOutput]:
        """
        Generates draft token IDs using the Eagle model for speculative decoding.

        This function constructs and processes the Eagle model inputs based on 
        the target model's output and attention metadata. It executes forward
        passes through the Eagle model to generate multiple speculative draft
        tokens.

        Let us consider this example to walk through the logic in the EagleProposer.
        Let us suppose we have 3 sequences S1 which is a completed prefill starting
        at the 0 position, S2 is a pratial prefill starting at non-zero position
        (not the first chunk) and S3 which is a decode.
        
        Target Model
        Sequences - [S1, S2, S3]
        Tokens - [T11, T12, T13, T14, T21, T22, T23, T31, T32, T33]
        Positions - [0,1,2,3,9,10,11,44,45,46]
        Hidden States -[H11,H12,H13,H14,H21,H22,H23,H31,H32,H33]
        Sampled Token Ids - [[T15],[],[T32]]
        Next Prompt Token Ids - [[],[T24],[]]


        First we use the hidden states, input positions and tokens from the
        target model to run a prefill in the Eagle to align the KV cache of
        the target model with that of Eagle. After the prefill we run
        forward passes with Eagle as needed for more speculative tokens.

        Eagle Prefill Forward Pass
        Sequences - [S1, S2, S3]
        Tokens - [T12, T13, T14, T15, T22, T23, T24, T32]
        Positions - [0,1,2,3,9,10,11,44]
        Previous Hidden States - [H11,H12,H13,H14,H21,H22,H23,H31]
        Hidden States -[E1,E2,E3,E4,E5,E6,E7,E8]
        Sampled Tokens - [[T16],[T25],[T33']]

        Note that for S1 we drop the token T11 which is the token
        at position 0. For S2 and S3 we don't need to consider
        tokens T21 and T31 respectively because they have already
        been processed in previous steps. Also note that a token positon
        in Eagle is always less by 1 compared to the target model since
        we always drop the token at position 0.

        Eagle speculation passes as needed.
        Sequences - [S1, S2, S3]
        Tokens - [T16, T25, T33']
        Positions - [4,12,45]
        Previous Hidden States - [E4,E7,E8]
        
        Args:
            target_model_input_ids: Input token IDs used in the target model.
            target_model_positions: Positional indices corresponding to 
                `target_model_input_ids`.
            target_model_hidden_states: Hidden states produced by the 
                target model during inference.
            target_model_seq_lens: List of sequence lengths in the 
                target model.
            sampled_token_ids: Previously sampled token IDs from the
                target model.
            next_prompt_token_ids: The next prompt token IDs for 
                each sequence, if available.
            is_prefill: Flags indicating whether each sequence is in 
                the prefill stage.
            num_draft_tokens_to_propose: Number of speculative draft tokens 
                to generate per sequence.
            attention_metadata: Attention Metadata that was used by the
                target model for its forward pass.

        Returns:
            list[SamplerOutput]: A list of sampled token outputs generated by 
            the Eagle model. The cardinality of the list will be equal to 
            num_draft_tokens_to_propose. Each SamplerOutput will contain
            sampled tokens for all sequences including the partial prefill
            ones.
        """
        # Calculate start locations for sequences and their respective lengths
        target_model_start_locs = [0] + list(
            itertools.accumulate(target_model_seq_lens))[:-1]
        # Calculate the expected sequence lengths and sequence start locations
        # in Eagle.
        # If this is a prefill then the expected Eagle sequence length will be
        # the same as that of the target model sequence. If it is a decode the
        # the expected sequence length will be the number of tokens accepted
        # by the target model.
        eagle_seq_lens = [
            target_model_seq_lens[i]
            if is_prefill[i] else len(sampled_token_ids[i])
            for i in range(len(target_model_seq_lens))
        ]
        eagle_num_tokens = sum(eagle_seq_lens)
        num_seqs = len(target_model_seq_lens)
        eagle_start_locs = [0] + list(
            itertools.accumulate(eagle_seq_lens))[:-1]

        # Convert lists to tensors for computation
        sampled_token_ids_tensors = [
            torch.tensor(tokens,
                         dtype=torch.int,
                         device=target_model_positions.device)
            for tokens in sampled_token_ids
        ]
        target_model_seq_lens_tensor = torch.tensor(
            target_model_seq_lens,
            dtype=torch.int,
            device=target_model_positions.device)
        eagle_start_locs_tensor = torch.tensor(
            eagle_start_locs,
            dtype=torch.int,
            device=target_model_positions.device)
        eagle_seq_lens_tensor = torch.tensor(
            eagle_seq_lens,
            dtype=torch.int,
            device=target_model_positions.device)

        # Initialize tensors for eagle model inputs based on the
        # expected eagle sequence lengths that we have computed.
        eagle_input_ids = torch.zeros(eagle_num_tokens,
                                      dtype=target_model_positions.dtype,
                                      device=target_model_input_ids.device)
        eagle_positions = torch.zeros(eagle_num_tokens,
                                      dtype=target_model_positions.dtype,
                                      device=target_model_positions.device)
        eagle_prev_hidden_states = torch.zeros(
            (eagle_num_tokens, target_model_hidden_states.shape[1]),
            dtype=target_model_hidden_states.dtype,
            device=target_model_hidden_states.device)
        eagle_slot_mappings = torch.zeros(eagle_num_tokens,
                                          dtype=target_model_positions.dtype,
                                          device=target_model_positions.device)

        # Populate the eagle model inputs for the first forward pass.
        for req_idx in range(len(eagle_seq_lens)):
            eagle_start_loc = eagle_start_locs[req_idx]
            eagle_seq_len = eagle_seq_lens[req_idx]
            target_model_start_loc = target_model_start_locs[req_idx]
            target_model_start_position = target_model_positions[
                target_model_start_loc]

            # Populate Eagle positions. The start positions will be the same
            # as the start position in the target model.
            eagle_positions[
                eagle_start_loc:eagle_start_loc + eagle_seq_len] =\
                    torch.arange(
                        target_model_start_position,
                        target_model_start_position + eagle_seq_len)

            # Populate previous hidden states to use in the Eagle model using
            # the hidden states of the target model.
            eagle_prev_hidden_states[
                eagle_start_loc:eagle_start_loc + eagle_seq_len] =\
                    target_model_hidden_states[
                        target_model_start_loc :\
                            target_model_start_loc + eagle_seq_len]

            # Populate the eagle slot mappings to be used in the Eagle
            # attention metadata.
            target_model_start_slot_position =\
                attention_metadata.slot_mapping[target_model_start_loc]
            eagle_slot_mappings[
                eagle_start_loc:eagle_start_loc + eagle_seq_len] =\
                torch.arange(
                    target_model_start_slot_position,
                    target_model_start_slot_position + eagle_seq_len)

            if is_prefill[req_idx]:
                # Drop the first input id. If this is a partial prefill then
                # use the next prompt token as the last token input else if it
                # is a completed prefill then use the  sampled token.
                eagle_input_ids[eagle_start_loc :\
                    eagle_start_loc + eagle_seq_len -1] =\
                        target_model_input_ids[
                            target_model_start_loc + 1 :\
                                target_model_start_loc + eagle_seq_len]
                eagle_input_ids[
                    eagle_start_loc +
                    eagle_seq_len] = next_prompt_token_ids[req_idx][0] if len(
                        next_prompt_token_ids[req_idx]
                    ) == 1 else sampled_token_ids_tensors[req_idx][0]
            else:
                # Non prefill sequences. For these sequences the inputs are
                # from the tokens sampled and accepted by the target model.
                eagle_input_ids[eagle_start_loc :\
                    eagle_start_loc + eagle_seq_len] = \
                        sampled_token_ids_tensors[req_idx][:eagle_seq_len]

        # Construct the attention metadata to use in the Eagle model
        eagle_attention_metadata = attention_metadata
        eagle_attention_metadata.num_actual_tokens = eagle_num_tokens
        # To compute the sequence lengths subtract the target model sequence
        # lengths for this forward pass and replace it with the expected
        # sequence lengths in the eagle model.
        eagle_attention_metadata.seq_lens =\
             eagle_attention_metadata.seq_lens -\
                 target_model_seq_lens_tensor + eagle_seq_lens_tensor
        eagle_attention_metadata.max_seq_len =\
            eagle_attention_metadata.seq_lens.max().item()
        eagle_attention_metadata.query_start_loc = torch.cat(
            [eagle_start_locs_tensor,
             torch.tensor([eagle_num_tokens])])
        eagle_attention_metadata.max_query_len =\
            eagle_seq_lens_tensor.max().item()
        eagle_attention_metadata.slot_mapping = eagle_slot_mappings

        # Compute logits indices. These are the indices corresponding
        # to the end of each sequence.
        logits_indices = (
            eagle_start_locs_tensor + eagle_seq_lens_tensor - 1
        )

        # Initialize result container
        result: list[SamplerOutput] = []

        # Perform forward pass through the Eagle model
        with set_forward_context(eagle_attention_metadata, self._vllm_config):
            eagle_hidden_states = self._model(
                input_ids=eagle_input_ids,
                positions=eagle_positions,
                previous_hidden_states=eagle_prev_hidden_states,
            )

            # Select hidden states for sampling
            sampled_hidden_states = eagle_hidden_states[logits_indices]

            # Compute logits and sample next tokens
            logits = self._model.compute_logits(sampled_hidden_states, None)
            sampler_output = self._model.sample(
                logits=logits,
                sampling_metadata=self._sampling_metadata,
            )
            result.extend(sampler_output)

        # Prepare inputs for the next forward pass of the Eagle model.
        # Update position IDs to be the last position in each sequence + 1.
        eagle_positions = eagle_positions[logits_indices] + 1
        # Construct input IDs using the sampled tokens from the previous step.
        eagle_input_ids = torch.stack([s.sampled_token_ids for s in sampler_output])
        # Use the hidden state from the previous forward pass as input.
        eagle_prev_hidden_states = eagle_hidden_states

        # Update attention metadata that is to be used for the next round of
        # speculations by the Eagle model.
        # The number of tokens in the next forward passes will be same 
        # as the number of input sequences i.e. one token for each sequence.
        eagle_attention_metadata.num_actual_tokens = num_seqs
        # Increment the sequence lengths by 1
        eagle_attention_metadata.seq_lens =\
            eagle_attention_metadata.seq_lens + 1
        eagle_attention_metadata.max_seq_len =\
            eagle_attention_metadata.seq_lens.max().item()
        # One token for each sequence. Set the query start locations
        # accordingly.
        eagle_attention_metadata.query_start_loc = torch.arange(
            0,
            num_seqs + 1,
            device=eagle_attention_metadata.seq_lens.device)
        eagle_attention_metadata.max_query_len = 1
        # Update the slot mapping to use for the next forward pass.
        # The updated slot mappings will be the last slot for each
        # sequence + 1.
        eagle_attention_metadata.slot_mapping =\
            eagle_attention_metadata.slot_mapping[logits_indices] + 1

        # Generate additional draft tokens as requested.
        for _ in range(num_draft_tokens_to_propose - 1):
            with set_forward_context(eagle_attention_metadata,
                                     self._vllm_config):
                eagle_hidden_states = self._model(
                    input_ids=eagle_input_ids,
                    positions=eagle_positions,
                    previous_hidden_states=eagle_prev_hidden_states,
                )
                logits = self._model.compute_logits(
                    eagle_hidden_states, None)
                sampler_output = self._model.sample(
                    logits=logits,
                    sampling_metadata=self._sampling_metadata,
                )
                result.extend(sampler_output)

                # Update the inputs to the Eagle model and the attention
                # metadata to be passed to the model.
                eagle_input_ids = sampler_output[0].sampled_token_ids
                eagle_positions = eagle_positions + 1
                eagle_prev_hidden_states = eagle_hidden_states
                eagle_attention_metadata.slot_mapping =\
                    eagle_attention_metadata.slot_mapping + 1
                eagle_attention_metadata.seq_lens =\
                    eagle_attention_metadata.seq_lens + 1
                eagle_attention_metadata.max_seq_len =\
                    eagle_attention_metadata.seq_lens.max().item()
        
        return result
