# SPDX-License-Identifier: Apache-2.0
"""
Example for manipulate(export/import) kv_cache blocks, and infer with engine.step()
"""

import copy
from typing import List, Tuple

import torch
from vllm import EngineArgs, RequestOutput, SamplingParams
from vllm.v1.core.kv_cache_utils import hash_request_tokens
from vllm.v1.engine.llm_engine import LLMEngine
from vllm.v1.request import Request

KV_CACHE_FILE = "kv_caches.pt"
REQUEST_ID = 0
# A prompt containing a large markdown table. The table is randomly generated by GPT-4.
LONG_PROMPT = (
    "You are a helpful assistant in recognizes the content of tables in markdown format. Here is a table as follows.\n# Table\n"
    + """
| ID  | Name          | Age | Occupation    | Country       | Email                  | Phone Number   | Address                       |
|-----|---------------|-----|---------------|---------------|------------------------|----------------|------------------------------|
| 1   | John Doe      | 29  | Engineer      | USA           | john.doe@example.com   | 555-1234       | 123 Elm St, Springfield, IL  |
| 2   | Jane Smith    | 34  | Doctor        | Canada        | jane.smith@example.com | 555-5678       | 456 Oak St, Toronto, ON      |
| 3   | Alice Johnson | 27  | Teacher       | UK            | alice.j@example.com    | 555-8765       | 789 Pine St, London, UK      |
| 4   | Bob Brown     | 45  | Artist        | Australia     | bob.b@example.com      | 555-4321       | 321 Maple St, Sydney, NSW    |
| 5   | Carol White   | 31  | Scientist     | New Zealand   | carol.w@example.com    | 555-6789       | 654 Birch St, Wellington, NZ |
| 6   | Dave Green    | 28  | Lawyer        | Ireland       | dave.g@example.com     | 555-3456       | 987 Cedar St, Dublin, IE     |
| 7   | Emma Black    | 40  | Musician      | USA           | emma.b@example.com     | 555-1111       | 246 Ash St, New York, NY     |
| 8   | Frank Blue    | 37  | Chef          | Canada        | frank.b@example.com    | 555-2222       | 135 Spruce St, Vancouver, BC |
| 9   | Grace Yellow  | 50  | Engineer      | UK            | grace.y@example.com    | 555-3333       | 864 Fir St, Manchester, UK   |
| 10  | Henry Violet  | 32  | Artist        | Australia     | henry.v@example.com    | 555-4444       | 753 Willow St, Melbourne, VIC|
| 11  | Irene Orange  | 26  | Scientist     | New Zealand   | irene.o@example.com    | 555-5555       | 912 Poplar St, Auckland, NZ  |
| 12  | Jack Indigo   | 38  | Teacher       | Ireland       | jack.i@example.com     | 555-6666       | 159 Elm St, Cork, IE         |
| 13  | Karen Red     | 41  | Lawyer        | USA           | karen.r@example.com    | 555-7777       | 357 Cedar St, Boston, MA     |
| 14  | Leo Brown     | 30  | Chef          | Canada        | leo.b@example.com      | 555-8888       | 246 Oak St, Calgary, AB      |
| 15  | Mia Green     | 33  | Musician      | UK            | mia.g@example.com      | 555-9999       | 975 Pine St, Edinburgh, UK   |
| 16  | Noah Yellow   | 29  | Doctor        | Australia     | noah.y@example.com     | 555-0000       | 864 Birch St, Brisbane, QLD  |
| 17  | Olivia Blue   | 35  | Engineer      | New Zealand   | olivia.b@example.com   | 555-1212       | 753 Maple St, Hamilton, NZ   |
| 18  | Peter Black   | 42  | Artist        | Ireland       | peter.b@example.com    | 555-3434       | 912 Fir St, Limerick, IE     |
| 19  | Quinn White   | 28  | Scientist     | USA           | quinn.w@example.com    | 555-5656       | 159 Willow St, Seattle, WA   |
| 20  | Rachel Red    | 31  | Teacher       | Canada        | rachel.r@example.com   | 555-7878       | 357 Poplar St, Ottawa, ON    |
| 21  | Steve Green   | 44  | Lawyer        | UK            | steve.g@example.com    | 555-9090       | 753 Elm St, Birmingham, UK   |
| 22  | Tina Blue     | 36  | Musician      | Australia     | tina.b@example.com     | 555-1213       | 864 Cedar St, Perth, WA      |
| 23  | Umar Black    | 39  | Chef          | New Zealand   | umar.b@example.com     | 555-3435       | 975 Spruce St, Christchurch, NZ|
| 24  | Victor Yellow | 43  | Engineer      | Ireland       | victor.y@example.com   | 555-5657       | 246 Willow St, Galway, IE    |
| 25  | Wendy Orange  | 27  | Artist        | USA           | wendy.o@example.com    | 555-7879       | 135 Elm St, Denver, CO       |
| 26  | Xavier Green  | 34  | Scientist     | Canada        | xavier.g@example.com   | 555-9091       | 357 Oak St, Montreal, QC     |
| 27  | Yara Red      | 41  | Teacher       | UK            | yara.r@example.com     | 555-1214       | 975 Pine St, Leeds, UK       |
| 28  | Zack Blue     | 30  | Lawyer        | Australia     | zack.b@example.com     | 555-3436       | 135 Birch St, Adelaide, SA   |
| 29  | Amy White     | 33  | Musician      | New Zealand   | amy.w@example.com      | 555-5658       | 159 Maple St, Wellington, NZ |
| 30  | Ben Black     | 38  | Chef          | Ireland       | ben.b@example.com      | 555-7870       | 246 Fir St, Waterford, IE    |
"""
)
RPOMPTS1 = [
    (
        LONG_PROMPT
        + "Question: what is the age of John Doe? Your answer: The age of John Doe is ",
        SamplingParams(temperature=0.0),
    ),
]
RPOMPTS2 = [
    (
        LONG_PROMPT
        + "Question: what is the age of Zack Blue? Your answer: The age of Zack Blue is ",
        SamplingParams(temperature=0.0),
    ),
]


def generate(
    engine: LLMEngine,
    prompts: List[Tuple[str, SamplingParams]],
    dump_kv_cache_block_num: int = 0,
    kv_cache_to_load=[],
):
    kv_caches_to_dump = []

    """Generates for the input prompts.

    Args:
        engine: vLLM engine
        prompts: input prompts
        dump_kv_cache_block_num: the length of kv_cache to dump, 0 for no dumping
        kv_cache_to_load: the kv_cache to load, shape is 
            [layer * torch.Size([2, num_block, block_size, head_num, head_hidden_states])],
            for internlm-1_8b is [layer * torch.Size([2, 24523, 16, 8, 128])]

    Returns:
        tuple[0]: text generated
        tuple[1]: kv_cache is kv_cache_to_load is not 0, shape is 
            [layer * torch.Size([2, num_block, block_size, head_num, head_hidden_states])]
    """
    while prompts or engine.has_unfinished_requests():
        if prompts:
            global REQUEST_ID
            request_id = str(REQUEST_ID)
            REQUEST_ID += 1
            prompt, sampling_params = prompts.pop(0)

            # ReImplement of engine.add_request()
            # engine.add_request(str(REQUEST_ID), prompt, sampling_params)

            # 1) Process raw inputs into the request.
            request = engine.processor.process_inputs(
                request_id,
                prompt,
                sampling_params,
                arrival_time=None,
                lora_request=None,
                trace_headers=None,
                prompt_adapter_request=None,
                priority=0,
            )
            # 2) Make a new RequestState and queue.
            engine.output_processor.add_request(request)

            # 3) Add the request to EngineCore.
            engine.engine_core.add_request(request)

        # load kv_cache
        if kv_cache_to_load:
            assert kv_cache_to_load, "no kv_cache_to_load"
            num_blocks = kv_cache_to_load[0].shape[1]

            engine_core = engine.engine_core.engine_core
            kv_cache_manager = engine_core.scheduler.kv_cache_manager
            kv_caches_old = (
                engine_core.model_executor.driver_worker.worker.model_runner.kv_caches
            )
            num_layers = len(kv_caches_old)

            # allocate new blocks
            new_blocks = kv_cache_manager._get_new_blocks(num_blocks)

            # hash results of the same tokens in 2 process are different
            block_hash_list = hash_request_tokens(
                kv_cache_manager.block_size, Request.from_engine_core_request(request)
            )

            # DO LOAD
            for i_blk in range(num_blocks):
                block_hash = block_hash_list[i_blk]
                block_id = new_blocks[i_blk].block_id

                # DO LOAD #1: update block_hash in block
                new_blocks[i_blk]._block_hash = block_hash
                # DO LOAD #2: update MAP[block_hash,block] in kv_cache_manager
                kv_cache_manager.cached_block_hash_to_block[block_hash][block_id] = (
                    new_blocks[i_blk]
                )

                for i_layer in range(num_layers):
                    # DO LOAD #3: copy data from kv_cache_to_load to new_blocks
                    kv_caches_old[i_layer][:, new_blocks[i_blk].block_id, :, :, :] = (
                        kv_cache_to_load[i_layer][:, i_blk, :, :, :]
                    )

        request_outputs: List[RequestOutput] = engine.step()

        # dump kv_cache
        if dump_kv_cache_block_num:
            engine_core = engine.engine_core.engine_core
            kv_cache_manager = engine_core.scheduler.kv_cache_manager
            # [layer * torch.Size([2, 24523, 16, 8, 128])]
            kv_caches_old = (
                engine_core.model_executor.driver_worker.worker.model_runner.kv_caches
            )
            req_blocks = kv_cache_manager.req_to_blocks[request.request_id]

            if len(req_blocks) > dump_kv_cache_block_num:
                num_layers = len(kv_caches_old)
                cache_shape = list(kv_caches_old[0].shape)
                # 1: dimension of num_block
                cache_shape[1] = dump_kv_cache_block_num

                kv_caches_to_dump = [
                    torch.zeros(cache_shape) for _ in range(num_layers)
                ]
                for i_layer in range(num_layers):
                    for i_blk, blk in enumerate(req_blocks[:dump_kv_cache_block_num]):
                        kv_caches_to_dump[i_layer][:, i_blk, :, :, :] = kv_caches_old[
                            i_layer
                        ][:, blk.block_id, :, :, :]

                # dump for only once (prefilling stage)
                dump_kv_cache_block_num = 0

        for request_output in request_outputs:
            if request_output.finished:
                print("OUTPUT of generate(): ", request_output.outputs[0].text)
                return request_output.outputs[0].text, kv_caches_to_dump


def run_auto_prefix_cache(engine):
    """Example for Auto Prefix Caching(vLLM APC)
    docs: https://docs.vllm.ai/en/latest/design/v1/prefix_caching.html
    """

    generate(engine, copy.deepcopy(RPOMPTS1))
    generate(engine, copy.deepcopy(RPOMPTS2))


def run_export_cache(engine):
    """Example for export kv_cache"""
    _, rsp_cache = generate(engine, copy.deepcopy(RPOMPTS1), dump_kv_cache_block_num=80)
    with open(KV_CACHE_FILE, "wb") as f:
        torch.save(rsp_cache, f)
    generate(engine, copy.deepcopy(RPOMPTS2))


def run_import_cache(engine):
    """Example for export kv_cache"""
    with open(KV_CACHE_FILE, "rb") as f:
        kv_cache_to_load = torch.load(f)

    generate(engine, copy.deepcopy(RPOMPTS1), kv_cache_to_load=kv_cache_to_load)
    generate(engine, copy.deepcopy(RPOMPTS2), kv_cache_to_load=kv_cache_to_load)


def run_engine1():
    """Example for export kv_cache with 1st engine

    generate() called for 4 times:
        #1: hit 0 cache blocks
        #2: hit 87 cache blocks, 87 blocks is for LONG_PROMPT
        #3: hit 88 cache blocks, 1 more block is for prompt appendding to LONG_PROMPT:
            "Question: what is the age of John Doe? Your answer: The age of John Doe is "
        #4: hit 88 cache blocks, 1 more block is for prompt appendding to LONG_PROMPT:
            "Question: what is the age of Zack Blue? Your answer: The age of Zack Blue is "

    Note: to check how many blocks hit, we can check the length of `computed_blocks` in Scheduler.schedule()
    """

    engine_args = EngineArgs(
        model="internlm2-1_8b", trust_remote_code=True, gpu_memory_utilization=0.4
    )
    engine = LLMEngine.from_engine_args(engine_args)

    print("=" * 60)
    print("run_auto_prefix_cache")
    print("=" * 60)
    run_auto_prefix_cache(engine)

    print("=" * 60)
    print("run_export_cache")
    print("=" * 60)
    run_export_cache(engine)


def run_engine2():
    """Example for import kv_cache with 2st engine, the 1st engine is filled with kv_cache

    generate() called for 2 times:
        #1: hit 80 cache blocks, we export/import only 80 blocks
        #2: hit 87 cache blocks, 87 blocks is for LONG_PROMPT

    Note: to check how many blocks hit, we can check the length of `computed_blocks` in Scheduler.schedule()
    """

    engine_args = EngineArgs(model="internlm2-1_8b", trust_remote_code=True)
    engine = LLMEngine.from_engine_args(engine_args)

    print("=" * 60)
    print("run_import_cache")
    print("=" * 60)
    run_import_cache(engine)


if __name__ == "__main__":
    run_engine1()
    run_engine2()
