list:
    @just --list

leader_address := "10.96.0.64" # nb1-h100-60
# hosts := "nb1-h100-60 nb1-h100-61"
# num_nodes := "2"
hosts := "nb1-h100-60 nb1-h100-61 nb1-h100-62 nb1-h100-63"
num_nodes := "4"
# hosts := "nb1-h100-60 nb1-h100-15 nb1-h100-45 nb1-h100-61 nb1-h100-62 nb1-h100-63"
# num_nodes := "6"

# Helper function to run commands on all hosts with proper signal handling
_run-on-all-hosts cmd:
    #!/usr/bin/env bash
    # Array to store background process PIDs
    pids=()
    
    # Function to cleanup background processes
    cleanup() {
        echo "Received interrupt signal. Terminating all SSH processes..."
        for pid in "${pids[@]}"; do
            if kill -0 "$pid" 2>/dev/null; then
                kill -TERM "$pid" 2>/dev/null
            fi
        done
        # Wait a bit for graceful termination, then force kill if needed
        sleep 2
        for pid in "${pids[@]}"; do
            if kill -0 "$pid" 2>/dev/null; then
                kill -KILL "$pid" 2>/dev/null
            fi
        done
        exit 1
    }
    
    # Set up trap for SIGINT (Ctrl+C) and SIGTERM
    trap cleanup INT TERM
    
    for host in {{hosts}}; do
        ssh $host "bash --login -c '{{cmd}}'" &
        pids+=($!)  # Store the PID of the background process
    done
    
    # Wait for all background processes
    wait
    echo "All processes completed successfully."

# Helper function to run indexed commands on all hosts with proper signal handling
_run-indexed-on-all-hosts cmd:
    #!/usr/bin/env bash
    # Array to store background process PIDs
    pids=()
    
    # Function to cleanup background processes
    cleanup() {
        echo "Received interrupt signal. Terminating all SSH processes..."
        for pid in "${pids[@]}"; do
            if kill -0 "$pid" 2>/dev/null; then
                kill -TERM "$pid" 2>/dev/null
            fi
        done
        # Wait a bit for graceful termination, then force kill if needed
        sleep 2
        for pid in "${pids[@]}"; do
            if kill -0 "$pid" 2>/dev/null; then
                kill -KILL "$pid" 2>/dev/null
            fi
        done
        exit 1
    }
    
    # Set up trap for SIGINT (Ctrl+C) and SIGTERM
    trap cleanup INT TERM
    
    i=0
    for host in {{hosts}}; do
        # Replace INDEX_PLACEHOLDER with actual index and HOST_PLACEHOLDER with actual host
        cmd_with_index="{{cmd}}"
        cmd_with_index="${cmd_with_index//INDEX_PLACEHOLDER/$i}"
        cmd_with_index="${cmd_with_index//HOST_PLACEHOLDER/$host}"
        ssh $host "bash --login -c '$cmd_with_index'" &
        pids+=($!)  # Store the PID of the background process
        ((i++))
    done
    
    # Wait for all background processes
    wait
    echo "All processes completed successfully."

run-commands-on-all-nodes:
    #!/usr/bin/env bash
    for host in {{hosts}}; do
        # ssh $host "bash -l -c 'hostname'" &
        # ssh $host "bash -l -c 'nvcc --version'" &
        # ssh $host "bash -l -c 'curl --proto =https --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y'" &
        # ssh $host "bash -l -c 'cargo install just'" &
        ssh $host "bash -l -c 'HF_TRANSFER=1 /mnt/data/home/smo/vllm/.venv/bin/huggingface-cli download deepseek-ai/DeepSeek-R1'" &
    done
    wait 

env_sh := "~/vllm/benchmarks/large-scale-benchmarks/env.sh"
pwd := "~/vllm/benchmarks/large-scale-benchmarks"

_worker-run-gdr-copy-test:
    #!/usr/bin/env bash
    source {{env_sh}}
    $WORKSPACE/gdrcopy_install/bin/gdrcopy_copybw

run-gdr-copy-test:
    @just _run-on-all-hosts "cd {{pwd}} && just _worker-run-gdr-copy-test"

_worker-run-deepep-intranode-test host:
    #!/usr/bin/env bash
    source {{env_sh}}
    cd ~/vllm
    source .venv/bin/activate
    python $WORKSPACE/DeepEP/tests/test_intranode.py | tee {{pwd}}/logs/deepep-intranode-test-{{host}}.log

run-deepep-intranode-test:
    @just _run-indexed-on-all-hosts "cd {{pwd}} && just _worker-run-deepep-intranode-test HOST_PLACEHOLDER"

_worker-run-deepep-internode-test i host:
    #!/usr/bin/env bash
    # set -ex
    source {{env_sh}}
    cd ~/vllm
    source .venv/bin/activate

    export RANK={{i}}
    export WORLD_SIZE={{num_nodes}}
    export MASTER_ADDR={{leader_address}}

    # export NCCL_DEBUG=INFO
    python $WORKSPACE/DeepEP/tests/test_internode.py 2>&1 | tee {{pwd}}/logs/deepep-internode-test-{{host}}.log

run-deepep-internode-test:
    @just _run-indexed-on-all-hosts "cd {{pwd}} && just _worker-run-deepep-internode-test INDEX_PLACEHOLDER HOST_PLACEHOLDER"

# Kill any existing vLLM processes across all nodes
kill-vllm-processes:
    #!/usr/bin/env bash
    echo "Killing vLLM processes on all nodes..."
    for host in {{hosts}}; do
        ssh $host "pkill -f 'vllm serve' || true; pkill -f 'python.*vllm' || true" &
    done
    wait
    echo "All vLLM processes killed."
    sleep 3


_worker-run-r1 i:
    #!/usr/bin/env bash
    source {{env_sh}}
    cd ~/vllm
    source .venv/bin/activate

    set -ex

    # Enhanced timeout and network configuration for DeepEP
    export VLLM_ALL2ALL_BACKEND="deepep_high_throughput" 
    export VLLM_USE_DEEP_GEMM=1
    export DP_SIZE=$(({{num_nodes}} * 8))
    export DP_SIZE_LOCAL=8
    export START_RANK=$(({{i}} * 8))


    # Get the actual IP address of this node
    export HOST=$(hostname -I | awk '{print $1}')
    
    # Enhanced network interface environment variables
    export GLOO_SOCKET_IFNAME=eth0
    # export NCCL_SOCKET_IFNAME=eth0
    # export NCCL_IB_DISABLE=0
    # export NCCL_NET_GDR_LEVEL=3
    # export NCCL_TIMEOUT=1800  # 30 minutes timeout for NCCL operations
    # export NCCL_BLOCKING_WAIT=1
    
    # DeepEP and distributed communication timeouts
    export VLLM_RPC_TIMEOUT=600000  # 10 minutes in milliseconds
    export DEEPEP_TIMEOUT=300  # 5 minutes timeout for DeepEP operations
    export TORCH_DISTRIBUTED_TIMEOUT=1800  # 30 minutes for torch distributed


    export CUDA_LAUNCH_BLOCKING=1
    export VLLM_LOGGING_LEVEL=DEBUG

    
    # PyTorch distributed debugging (optional - comment out for less verbose output)
    # export TORCH_DISTRIBUTED_DEBUG=DETAIL
    # export NCCL_DEBUG=INFO
    
    # Ray timeout configuration if using Ray backend
    # export RAY_CGRAPH_get_timeout=600
    
    # Memory and performance tuning
    # export CUDA_DEVICE_MAX_CONNECTIONS=1
    # export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
    
    # Only the leader node (index 0) should specify data-parallel-address
    # Worker nodes connect to the leader but don't bind to its address
    if [ {{i}} -eq 0 ]; then
        LEADER_OR_WORKER_ARGS=""
        DATA_PARALLEL_ADDRESS_ARG="--data-parallel-address {{leader_address}}"
        echo "Starting LEADER node (rank {{i}}) at {{leader_address}}"
    else
        LEADER_OR_WORKER_ARGS="--headless"
        DATA_PARALLEL_ADDRESS_ARG="--data-parallel-address {{leader_address}}"
        echo "Starting WORKER node (rank {{i}}) connecting to {{leader_address}}"
    fi

    # Add longer startup delay for worker nodes to ensure leader is ready
    if [ {{i}} -ne 0 ]; then
        sleep_time=$(({{i}} * 10))  # Stagger worker startup
        echo "Worker node {{i}} waiting ${sleep_time} seconds for leader..."
        sleep $sleep_time
    fi

    echo "Starting vLLM server with the following configuration:"
    echo "  Node rank: {{i}}"
    echo "  DP_SIZE: $DP_SIZE"
    echo "  START_RANK: $START_RANK"
    echo "  Host IP: $HOST"
    echo "  Leader address: {{leader_address}}"

    # --max-num-seqs 1 \
    # --max-model-len 2048 \
    # --max-seq-len 2048 \
    # --max-num-batched-tokens 2048 \
    
    # Start vLLM with enhanced error handling and timeouts
    vllm serve deepseek-ai/DeepSeek-R1 --trust-remote-code \
        --no-enable-prefix-caching --disable-log-requests \
        --tensor-parallel-size 1 \
        --enable-expert-parallel \
        --data-parallel-size $DP_SIZE \
        --data-parallel-size-local $DP_SIZE_LOCAL \
        $DATA_PARALLEL_ADDRESS_ARG \
        --data-parallel-rpc-port 5555 \
        --data-parallel-start-rank $START_RANK \
        --enforce-eager \
        --gpu-memory-utilization 0.8 \
        $LEADER_OR_WORKER_ARGS 2>&1 | tee {{pwd}}/logs/r1-rank-{{i}}.log

# Enhanced R1 runner with better error handling and timeouts
run-r1:
    @just kill-vllm-processes
    @just _run-indexed-on-all-hosts "cd {{pwd}} && just _worker-run-r1 INDEX_PLACEHOLDER"
