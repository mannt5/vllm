.. _auto_quant:

AutoQuant
==================


AutoQuant supports direct quantization deployment from fp16/bf16 models to llm_int8, smoothquant and 4 bit weight-only inference. It also supports model deployment after loading autoawq quantification. The above four deployment methods will be described in detail below.

Or you can quantize your own models by installing AutoAWQ or picking one of the `400+ models on Huggingface <https://huggingface.co/models?sort=trending&search=awq>`_. 

Install bitsandbytes to support llm_int8 and smoothquant inference.

.. code-block:: console

    $ pip install bitsandbytes

In order to use this quantification feature, you can add a configuration file named quant_config.json to the folder of your model. The file generated by AutoAWQ can be used directly. The file format is as follows:

.. code-block:: json-object

    {
        "zero_point": true,
        "q_group_size": 128,
        "w_bit": 4,
        "version": "gemm",
        "from_float": true,
        "quant_mode": "weight_only"
    }

"w_bit" can be 4 or 8 bit, quant_mode can be one of ["llm.int8", "smoothquant", "weight_only"], currently "weight_only" only supports 4 bit. "llm_int8" means LLM.int8() method that is the combination of vector-wise quantization and mixed precision decomposition. "smoothquant" means that the activation and weight use the vector-wise int8 quantification. "weight_only" Indicates that just the weight is converted into int4. "from_float" means that the weights of the model are stored in floating point format, and when the model needs to be loaded, it is dynamically converted into the target format and sent to the GPU for execution.
If it is a pure int4 model obtained from AutoAWQ, just configure it directly as follows:

.. code-block:: json-object

    {
        "zero_point": true,
        "q_group_size": 128,
        "w_bit": 4,
        "version": "gemm",
    }

The configuration can also be written in the configuration file of huggingface (config.json), in the following format:

.. code-block:: json-object

    "quantization_config": {
        "bits": 4,
        "group_size": 128,
        "quant_method": "autoquant",
        "version": "gemm",
        "zero_point": true,
        "from_float": true,
        "quant_mode": "weight_only"
    }

Here is an example of how to enable this feature:

.. code-block:: python

    from vllm import LLM, SamplingParams
    # Sample prompts.
    prompts = [
        "Hello, my name is",
        "The president of the United States is",
        "The capital of France is",
        "The future of AI is",
    ]
    # Create a sampling params object.
    sampling_params = SamplingParams(temperature=0.8, top_p=0.95)
    # Create an LLM.
    llm = LLM(model="/data/opt-125m", quantization="autoquant")
    # In order to use the autoquant quantification feature, 
    # there must be a correctly configured quant_config.json file
    # in the /data/opt-125m directory.
    # Generate texts from the prompts. The output is a list of RequestOutput objects
    # that contain the prompt, generated text, and other information.
    outputs = llm.generate(prompts, sampling_params)
    # Print the outputs.
    for output in outputs:
        prompt = output.prompt
        generated_text = output.outputs[0].text
        print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")

.. warning::

   Please note that "llm_int8" or "smoothquant" mode can only run in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.